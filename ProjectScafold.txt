# fan-timeline — Ready‑to‑run scaffold

Below is a copy‑pasteable scaffold you can drop into a new repo named `fan-timeline/`. Files are grouped by path. Wherever you see `# --- file: ...` that’s the start of a new file.

> Tip: Create the folders first, then paste each file into place. After setup, you can generate a **synthetic sample game** and run the demo immediately (no external data needed). Later, replace with real Reddit+PBP data.

---

# --- file: README.md

# Sports Fan Thread → Timeline + Sentiment

Turn chaotic live Reddit NBA game threads into a **clean, minute-by-minute fan timeline** with sentiment and top themes. This repo ships a full pipeline:

* **Teacher pipeline** to synthesize SFT data (no manual labels)
* **QLoRA SFT** script (Llama-3.1-8B / Qwen2-7B)
* **Schema validator & guardrails** to enforce strict JSON
* **FastAPI serve** + **Streamlit demo**
* **Synthetic data generator** so the demo runs out-of-the-box

## Quickstart

```bash
# 1) Create and activate a venv
python3 -m venv .venv && source .venv/bin/activate

# 2) Install deps
pip install --upgrade pip
pip install -r requirements.txt

# 3) Make sure Python can import src/
export PYTHONPATH=$(pwd)

# 4) Generate a synthetic sample game (no internet)
python src/timeline/make_synth.py --out data/sample

# 5) Build teacher outputs & SFT pairs
python src/timeline/teacher.py --reddit data/sample/reddit/2019-12-01-LAL-DAL.jsonl \
  --pbp data/sample/pbp/2019-12-01-LAL-DAL.json \
  --out data/sample/teacher

python src/timeline/make_sft.py --windows data/sample/teacher/windows.jsonl \
  --out data/sft/sft_data.jsonl

# 6) (Optional) Train with QLoRA (requires GPU; otherwise skip)
# Edit configs/nba.yaml if needed
python src/timeline/train_sft.py --config configs/nba.yaml

# 7) Run the Streamlit demo
streamlit run app/streamlit_app.py

# 8) Or run the API server
uvicorn src.timeline.serve:app --reload --port 8000
```

### Minimal hardware

* CPU runs for teacher & demo; training recommended on a modest GPU (8–24GB). Mac CPU fine for demo (model fallback uses teacher summarizer).

### Data folders

```
data/
  reddit/        # your Reddit game threads (jsonl per game)
  pbp/           # play-by-play json per game
  sft/           # synthesized SFT pairs
  sample/        # synthetic example generated by make_synth.py
```

### What to replace when using real data

* Put your Reddit threads into `data/reddit/` as `.jsonl` (one JSON object per comment with `body`, `created_utc`, `score`, `author`)
* Put your play-by-play into `data/pbp/` as `.json` (list of events with `period`, `clock`, `type`, `team`, `points`, `desc`)
* Adjust `configs/nba.yaml` thresholds & paths

---

# --- file: requirements.txt

# Core

pydantic>=2.7
fastapi>=0.111
uvicorn>=0.30
streamlit>=1.37
numpy>=1.26
pandas>=2.2
scikit-learn>=1.5
vaderSentiment>=3.3.2
jsonschema>=4.23

# ML (optional but recommended)

transformers>=4.42
accelerate>=0.33
peft>=0.11
trl>=0.9
datasets>=2.20

# bitsandbytes is optional and CUDA/Linux specific; skip on macOS if it fails

# bitsandbytes>=0.43

# Utils

tqdm>=4.66
pyyaml>=6.0.1

---

# --- file: configs/nba.yaml

# Paths

paths:
reddit\_dir: data/reddit
pbp\_dir: data/pbp
sft\_out: data/sft/sft\_data.jsonl
schema\_path: schema/timeline.schema.json

# Windowing

window:
seconds: 60            # 60-second windows
overlap: false
max\_chars: 3500        # cap window text
top\_k\_upvoted: 8       # take top-K by score
sample\_extra: 12       # plus a sample of others

# Sentiment thresholds

sentiment:
tau\_pos: 0.15
tau\_neg: -0.15
trim\_percentile: 0.9   # trim extreme users

# Teacher writer rules

teacher:
max\_event\_len: 120
include\_quote: true

# Training (QLoRA)

train:
base\_model: meta-llama/Meta-Llama-3.1-8B-Instruct   # or Qwen/Qwen2-7B-Instruct
output\_dir: outputs/sft-llama3-8b
epochs: 3
lr: 2e-4
lora\_rank: 16
lora\_alpha: 32
dropout: 0.05
max\_seq\_len: 3072
micro\_batch\_size: 1
gradient\_accumulation: 8

---

# --- file: schema/timeline.schema.json

{
"\$schema": "[https://json-schema.org/draft/2020-12/schema](https://json-schema.org/draft/2020-12/schema)",
"type": "object",
"required": \["game\_id", "timeline"],
"additionalProperties": false,
"properties": {
"game\_id": {"type": "string"},
"timeline": {
"type": "array",
"items": {
"type": "object",
"required": \["ts", "event", "fan\_sentiment"],
"additionalProperties": false,
"properties": {
"ts": {"type": "string", "pattern": "^(Q\[1-4]|OT\d?)\s\d{2}:\d{2}\$|^\d{2}:\d{2}–\d{2}:\d{2}\$"},
"event": {"type": "string", "minLength": 4, "maxLength": 160},
"fan\_sentiment": {"type": "string", "enum": \["pos", "neg", "mixed"]}
}
}
},
"top\_themes": {"type": "array", "items": {"type": "string"}, "maxItems": 6},
"notes": {"type": "string"}
}
}

---

# --- file: src/timeline/**init**.py

**all** = \[]

---

# --- file: src/timeline/utils.py

from **future** import annotations
import re
from typing import List, Dict

CLEAN\_RE = re.compile(r"\s+")
URL\_RE = re.compile(r"https?://\S+")
CODE\_RE = re.compile(r"`[^`]+\`")
QUOTE\_RE = re.compile(r">+\s.\*\$", re.MULTILINE)

def clean\_text(s: str) -> str:
"""Lightweight cleaning for Reddit comment bodies."""
s = URL\_RE.sub("", s)
s = CODE\_RE.sub("", s)
s = QUOTE\_RE.sub("", s)
s = s.replace("\u00a0", " ")
s = CLEAN\_RE.sub(" ", s).strip()
return s

def truncate\_tokens(s: str, max\_chars: int) -> str:
return s if len(s) <= max\_chars else s\[:max\_chars]

def mmss\_to\_secs(mmss: str) -> int:
m, s = mmss.split(":")
return int(m) \* 60 + int(s)

def secs\_to\_mmss(x: int) -> str:
m = x // 60
s = x % 60
return f"{m:02d}:{s:02d}"

---

# --- file: src/timeline/ingest\_reddit.py

from **future** import annotations
import json, argparse
from dataclasses import dataclass
from typing import List, Dict
from .utils import clean\_text

@dataclass
class Comment:\n    body: str
created\_utc: int
score: int
author: str

def load\_reddit\_jsonl(path: str) -> List\[Comment]:
out: List\[Comment] = \[]
with open(path, "r", encoding="utf-8") as f:
for line in f:
if not line.strip():
continue
obj = json.loads(line)
body = clean\_text(obj.get("body", ""))
if not body:
continue
out.append(Comment(
body=body,
created\_utc=int(obj.get("created\_utc", 0)),
score=int(obj.get("score", 0)),
author=str(obj.get("author", "")),
))
return out

def main():
ap = argparse.ArgumentParser()
ap.add\_argument("--infile", required=True)
ap.add\_argument("--outfile", required=True)
args = ap.parse\_args()
comments = load\_reddit\_jsonl(args.infile)
with open(args.outfile, "w", encoding="utf-8") as f:
for c in comments:
f.write(json.dumps(c.**dict**, ensure\_ascii=False) + "\n")

if **name** == "**main**":
main()

---

# --- file: src/timeline/parse\_pbp.py

from **future** import annotations
import json, argparse
from dataclasses import dataclass
from typing import List, Dict
from .utils import mmss\_to\_secs

@dataclass
class PbpEvent:
period: int            # 1..4 (OT as 5+)
clock: str             # MM\:SS (game clock)
team: str              # e.g., LAL/DAL
points: int            # points on this event
desc: str              # description

def load\_pbp\_json(path: str) -> List\[PbpEvent]:
with open(path, "r", encoding="utf-8") as f:
raw = json.load(f)
events: List\[PbpEvent] = \[]
for e in raw:
period = int(e.get("period", 1))
clock = str(e.get("clock", "12:00"))
team = str(e.get("team", ""))
points = int(e.get("points", 0))
desc = str(e.get("desc", ""))
events.append(PbpEvent(period, clock, team, points, desc))
return events

def detect\_big\_runs(events: List\[PbpEvent], window\_secs: int = 120) -> List\[Dict]:
\# Simple heuristic: any 8-0 (or more) within rolling 2-min
\# Returns list of dicts: {period, clock, run\_str}
out = \[]
\# Build timeline in seconds remaining within period
pts\_by\_time = \[]
for e in events:
t = mmss\_to\_secs(e.clock)
delta = e.points if e.team else 0
pts\_by\_time.append((e.period, t, e.team, delta))
\# naive scan
for i in range(len(pts\_by\_time)):
p0, t0, team0, \_ = pts\_by\_time\[i]
sum\_team = {team0: 0}
for j in range(i, len(pts\_by\_time)):
p, t, team, delta = pts\_by\_time\[j]
if p != p0 or t < t0 - window\_secs:
break
if team:
sum\_team\[team] = sum\_team.get(team, 0) + delta
for tm, pts in sum\_team.items():
if pts >= 8:
out.append({"period": p0, "clock": f"{t0//60:02d}:{t0%60:02d}", "run": f"{tm} {pts}-0"})
return out

if **name** == "**main**":
ap = argparse.ArgumentParser()
ap.add\_argument("--infile", required=True)
ap.add\_argument("--print\_runs", action="store\_true")
args = ap.parse\_args()
ev = load\_pbp\_json(args.infile)
if args.print\_runs:
print(detect\_big\_runs(ev))

---

# --- file: src/timeline/align\_time.py

from **future** import annotations
from typing import List, Dict, Tuple
from dataclasses import dataclass
import math

@dataclass
class TimeAlignConfig:
start\_utc: int    # scheduled tip-off in UTC seconds
q\_seconds: int = 12 \* 60

def map\_real\_to\_game(utc\_ts: int, cfg: TimeAlignConfig) -> Tuple\[str, str]:
"""Map absolute unix time to (quarter label, MM\:SS clock).
If mapping is uncertain/negative, return real-time bin label instead: "00:00–00:59" style.
"""
delta = utc\_ts - cfg.start\_utc
if delta < 0:
\# pregame chatter → use real-time bins
mm = max(0, delta // 60)
return ("REAL", f"{mm:02d}:00–{mm:02d}:59")
q = delta // cfg.q\_seconds
if q > 5:
\# postgame
mm = delta // 60
return ("REAL", f"{mm:02d}:00–{mm:02d}:59")
within = delta % cfg.q\_seconds
\# Convert to game clock (counting down)
remain = cfg.q\_seconds - within
m = remain // 60
s = remain % 60
return (f"Q{min(q+1, 4) if q < 4 else 'OT'}", f"{m:02d}:{s:02d}")

---

# --- file: src/timeline/windowing.py

from **future** import annotations
from typing import List, Dict, Tuple
import random
from .utils import truncate\_tokens

def build\_windows(comments: List\[Dict], start\_utc: int, seconds: int = 60,
max\_chars: int = 3500, top\_k\_upvoted: int = 8, sample\_extra: int = 12) -> List\[Dict]:
"""Group comments into minute windows; select top-K by score plus a sample of others."""
\# Bucket by minute from start
buckets: Dict\[int, List\[Dict]] = {}
for c in comments:
t = max(0, int(c\["created\_utc"]) - start\_utc)
minute = t // seconds
buckets.setdefault(minute, \[]).append(c)

```
windows = []
for minute, group in sorted(buckets.items()):
    group = sorted(group, key=lambda x: x.get("score", 0), reverse=True)
    topk = group[:top_k_upvoted]
    rest = group[top_k_upvoted:]
    sample = random.sample(rest, k=min(len(rest), sample_extra)) if rest else []
    chosen = topk + sample
    text = "\n".join(f"• {c['body']}" for c in chosen)
    text = truncate_tokens(text, max_chars)
    win = {
        "minute": int(minute),
        "window_label": f"{minute:02d}:00–{minute:02d}:59",
        "comments": chosen,
        "comments_text": text,
    }
    windows.append(win)
return windows
```

---

# --- file: src/timeline/teacher.py

from **future** import annotations
import os, json, argparse, math
from typing import List, Dict
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from .ingest\_reddit import load\_reddit\_jsonl
from .parse\_pbp import load\_pbp\_json
from .utils import secs\_to\_mmss

analyzer = SentimentIntensityAnalyzer()

LEXICON\_POS = {"clutch", "blocks", "dunk", "heat", "streak", "cooking", "run"}
LEXICON\_NEG = {"cold", "brick", "collapse", "turnover", "foul", "miss"}

def aggregate\_sentiment(comments: List\[Dict], trim: float = 0.9) -> float:
if not comments:
return 0.0
scored = \[]
for c in comments:
vs = analyzer.polarity\_scores(c.get("body", ""))
scored.append((vs\["compound"], max(1, int(c.get("score", 0)))))
\# upvote-weighted trimmed mean
scored.sort(key=lambda x: x\[0])
k = int(len(scored) \* (1 - trim))
core = scored\[k: len(scored) - k] if len(scored) > 2\*k else scored
num = sum(s \* w for s, w in core)
den = sum(w for \_, w in core)
return num / max(1, den)

def label\_sentiment(val: float, tau\_pos: float, tau\_neg: float) -> str:
if val >= tau\_pos:
return "pos"
if val <= tau\_neg:
return "neg"
return "mixed"

def write\_event(window: Dict, max\_len: int = 120) -> str:
\# Very simple rule-based writer using lexical hints
text = window\.get("comments\_text", "")
phrase = "run" if ("8-0" in text or "10-0" in text or "run" in text) else "sequence"
sent = aggregate\_sentiment(window\.get("comments", \[]))
if sent > 0.15:
core = "Home surge with timely scores"
elif sent < -0.15:
core = "Opponents go on a big run; costly mistakes"
else:
core = "Both sides trade buckets; momentum unclear"
\# Try to pick a short quote
quote = None
for c in window\.get("comments", \[])\[:5]:
b = c.get("body", "")
if 6 <= len(b) <= 60 and ("run" in b or "cooking" in b or "cold" in b):
quote = b
break
if quote:
out = f"{core}. {quote}"
else:
out = core
return out\[:max\_len]

def windows\_from\_sources(reddit\_path: str, pbp\_path: str, start\_minute: int = 0):
comments = \[c.**dict** for c in load\_reddit\_jsonl(reddit\_path)]
pbp = \[e.**dict** for e in load\_pbp\_json(pbp\_path)]
\# naive start time: use the earliest comment as minute 0
if comments:
start\_utc = min(c\["created\_utc"] for c in comments)
else:
start\_utc = 0
\# build minute windows
from .windowing import build\_windows
wins = build\_windows(comments, start\_utc=start\_utc)
return wins, pbp

def main():
ap = argparse.ArgumentParser()
ap.add\_argument("--reddit", required=True)
ap.add\_argument("--pbp", required=True)
ap.add\_argument("--out", required=True)
ap.add\_argument("--tau\_pos", type=float, default=0.15)
ap.add\_argument("--tau\_neg", type=float, default=-0.15)
args = ap.parse\_args()

```
os.makedirs(args.out, exist_ok=True)

wins, pbp = windows_from_sources(args.reddit, args.pbp)

# Build teacher windows with labels
out_windows = []
for w in wins:
    sent_val = aggregate_sentiment(w.get("comments", []))
    sent_lbl = label_sentiment(sent_val, args.tau_pos, args.tau_neg)
    event = write_event(w, max_len=120)
    # Time label: fallback to real-time window label
    ts = w["window_label"]
    out_windows.append({
        "ts": ts,  # could be replaced with Qx MM:SS if you have alignment
        "event": event,
        "fan_sentiment": sent_lbl,
        "comments_text": w.get("comments_text", ""),
    })

# Persist
with open(os.path.join(args.out, "windows.jsonl"), "w", encoding="utf-8") as f:
    for w in out_windows:
        f.write(json.dumps(w, ensure_ascii=False) + "\n")

# Theme extraction (very simple TF counts for demo)
from c
```
